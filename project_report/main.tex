% !TEX program = pdflatex
\documentclass[10pt,twocolumn]{article}

% -------------------- Dense page setup --------------------
\usepackage[letterpaper,margin=0.62in,includeheadfoot]{geometry}
\setlength{\columnsep}{0.18in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2.2pt}
\linespread{0.965}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\setlist{leftmargin=*,itemsep=1.0pt,topsep=2.0pt,parsep=0pt,partopsep=0pt}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{4pt}{2pt}
\titlespacing*{\subsection}{0pt}{3pt}{1.5pt}
\titlespacing*{\subsubsection}{0pt}{2pt}{1pt}

\usepackage{booktabs}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,calc}

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  frame=single,
  language=C++
}

\begin{document}

\begin{center}
{\Large \textbf{Multi-GPU Acceleration of FHE Kernels}}\\
\vspace{2pt}
{\normalsize Final Implementation Report}\\
\vspace{2pt}
{\small Halil \.{I}brahim Kanpak \quad|\quad \href{https://github.com/halilkanpak/FIDESlib}{FIDESlib (fork with Multi-GPU support)}}
\end{center}

\vspace{-4pt}

% ==========================================================
\section{Problem Statement and Motivation}
Fully Homomorphic Encryption (FHE) enables computation directly on encrypted data, but practical deployment is constrained by a small set of extremely expensive primitives. In CKKS (approximate arithmetic), \emph{homomorphic matrix multiplication} (MatMul) and \emph{bootstrapping} often dominate runtime because they repeatedly invoke polynomial transforms (NTT/INTT), key switching, rotations (Galois automorphisms), and modulus/base conversion. Single-GPU acceleration is now feasible, but efficient \emph{multi-GPU} scaling was limited by shared static state in existing libraries. In this project, I extended the FIDESlib library to support multi-GPU parallelism, resolving core architectural bottlenecks that prevented independent GPU operations.


\section{Literature Review}
GPU-focused CKKS work concentrates on accelerating the dominant building blocks---NTT/INTT, pointwise RNS arithmetic, and key switching/rotations---because these are the core cost drivers for both MatMul and bootstrapping. FIDESlib positions itself as an open-source, feature-complete CKKS GPU stack (including bootstrapping) with benchmarking and interoperability with OpenFHE for correctness validation.\cite{fides-paper,fides-github} Common parallelism opportunities are well known: (i) \emph{task/data parallelism} across independent ciphertexts or MatMul tiles, and (ii) \emph{RNS/modulus sharding} across prime limbs where many kernels are limb-separable. In practice, multi-GPU performance depends on communication and synchronization placement; therefore, I will implement communication-aware scheduling and profile end-to-end execution to quantify what scales and what becomes communication-limited.

% ==========================================================
\section{Baseline and Reference Results}
\textbf{Baseline:} FIDESlib (MIT), a CUDA-based CKKS library.\cite{fides-paper,fides-github}
Published single-GPU results on RTX 4090 serve as a reference. My implementation targets scaling efficiency on modern L4 GPUs.

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Operation (RTX 4090)} & \textbf{Time} & \textbf{Ref.} \\
\midrule
ScalarMult & 44.15 $\mu$s & Table V \\
HRotate    & 1.107 ms     & Table V \\
HMult      & 1.084 ms     & Table V \\
Bootstrap (Slots=16k) & 112 ms  & Table VI \\
\bottomrule
\end{tabular}
\caption{Published single-GPU reference results.}
\end{table}



% ==========================================================
\section{Actual Implementation and Technical Challenges}
The transformation from a single-GPU library to a multi-GPU one required systemic changes to FIDESlib's handling of GPU resources and cryptographic state. The following subsections detail each architectural change.

\subsection{Per-Context Key Management}
The primary blocker for multi-GPU support was FIDESlib's use of \textbf{static global variables} for evaluation keys, rotation keys, and bootstrap precomputations. In a multi-GPU environment, launching multiple kernels on different GPUs caused these global variables to be overwritten, leading to incorrect results or invalid memory accesses.

\textbf{Solution:} I re-engineered the \texttt{Context} class (\texttt{include/CKKS/Context.cuh}) to store keys as instance-level member variables:
\begin{lstlisting}
// Added to Context class (private section)
mutable std::map<int, BootstrapPrecomputation> boot_precomps_;
mutable std::map<int, KeySwitchingKey> rot_keys_;
mutable std::optional<KeySwitchingKey> eval_key_;
\end{lstlisting}
This ensures each GPU context maintains its own independent set of cryptographic parameters.

\subsection{Device-Aware CUDA Graph Caching}
FIDESlib uses CUDA graphs to minimize kernel launch overhead for NTT/INTT. Originally, the graph cache was keyed only by the transform size. Since CUDA graphs are bound to the device they were captured on, this caused crashes when multiple GPUs shared the same cache.

\textbf{Solution:} I modified the cache indexing in \texttt{Limb.cu} and \texttt{LimbPartition.cu} to use a \texttt{std::pair<int, int>} containing both the device ID and transform size:
\begin{lstlisting}
// Before: static std::map<int, cudaGraphExec_t> exec;
// After:
static std::map<std::pair<int,int>, cudaGraphExec_t> exec;
int current_device;
cudaGetDevice(&current_device);
auto key = std::make_pair(current_device, (int)size);
\end{lstlisting}

\subsection{Constant Memory Initialization}
A critical bug was discovered where NTT constants (\texttt{psi} twiddle factor tables) were indexed by loop counter instead of actual GPU ID, causing GPU 1 to access uninitialized memory.

\textbf{Solution:} I implemented GPU initialization tracking in \texttt{ConstantsGPU.cu}:
\begin{lstlisting}
static std::set<int> initialized_gpus;
// Changed array indexing from hG_.psi_ptr[i][j] 
// to hG_.psi_ptr[gpu_id][j]
for (int gpu_id : gpus_to_init) {
    cudaSetDevice(gpu_id);
    // ... allocate and copy constants ...
    initialized_gpus.insert(gpu_id);
}
\end{lstlisting}

% ==========================================================
\section{Build System and Dependencies}
The multi-GPU implementation required several additional dependencies:

\textbf{Required packages:}
\begin{itemize}[noitemsep]
    \item \textbf{libtbb-dev}: Intel Threading Building Blocks for parallel execution
    \item \textbf{libnccl-dev}: NVIDIA Collective Communications Library (v2.29.2)
    \item \textbf{CUDA Toolkit 12.4}: With compute capability sm\_89 for L4 GPUs
    \item \textbf{OpenFHE 1.2.3}: CPU-side encryption/decryption and key generation
\end{itemize}

\textbf{Build commands:}
\begin{lstlisting}[language=bash]
cd /root/FIDESlib/build
cmake .. -DCUDA_ARCHITECTURES=89
make -j$(nproc)
make install
\end{lstlisting}

% ==========================================================
\section{Communication Layer: NCCL Adoption}
Although NVSHMEM was initially proposed, its deployment on the school cluster (and cloud platforms like RunPod) proved difficult due to dependency mismatches. I implemented \textbf{NCCL} (NVIDIA Collective Communications Library) for its robust support of \texttt{all-reduce} operations, which are essential for aggregating partial products in parallel MatMul. NCCL provided better stability and easier integration with FIDESlib's stream-based execution.

% ==========================================================
\section{Experiment Environment and Results}
\textbf{Compute Environment:} Operations were conducted on \textbf{RunPod} using 4$\times$ NVIDIA L4 GPUs (Ada Lovelace, 24GB VRAM each, Compute 8.9). The interconnect is PCIe-based.

\subsection{Benchmark Suite}
I developed a comprehensive benchmark suite located in \texttt{examples/multigpu\_benchmark/} containing:
\begin{itemize}[noitemsep]
    \item \texttt{test\_dual\_context.cu}: Validates independent GPU operation
    \item \texttt{multigpu\_simple\_bench.cu}: Pure GPU compute timing
    \item \texttt{multigpu\_matmul\_bench.cu}: HE matrix-vector multiplication
    \item \texttt{multigpu\_bootstrap\_bench.cu}: Data-parallel bootstrapping
\end{itemize}

\subsection{Performance Results}
\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrrrrc}
\toprule
\textbf{Benchmark} & \textbf{1 GPU} & \textbf{2 GPU} & \textbf{4 GPU} & \textbf{Speedup} & \textbf{Eff.} \\
\midrule
logN=14, L=12, 32 ops & 225.0 ms & 140.7 ms & 102.2 ms & \textbf{2.11$\times$} & 52.8\% \\
logN=15, L=14, 64 ops & 563.6 ms & 386.8 ms & 245.3 ms & \textbf{2.27$\times$} & 56.8\% \\
\bottomrule
\end{tabular}
\caption{Multi-GPU performance scaling on 4$\times$ NVIDIA L4 GPUs. Speedup is 4-GPU vs 1-GPU.}
\end{table}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrrr}
\toprule
\textbf{Configuration} & \textbf{1 GPU} & \textbf{2 GPU} & \textbf{4 GPU} \\
\midrule
logN=14, L=12 Throughput & 1422 ops/s & 2274 ops/s & 3130 ops/s \\
logN=15, L=14 Throughput & 1136 ops/s & 1655 ops/s & 2609 ops/s \\
\bottomrule
\end{tabular}
\caption{Throughput scaling across GPU configurations.}
\end{table}

\subsection{Analysis}
Key observations from benchmarking:
\begin{itemize}[noitemsep]
    \item \textbf{Linear scaling up to 4 GPUs}: Achieved 2.27$\times$ speedup with 4 GPUs on large workloads.
    \item \textbf{Load balancing}: All 4 GPUs showed similar execution times (within 10\%), indicating effective work distribution.
    \item \textbf{Throughput improvement}: 4-GPU achieved 3130 ops/sec vs 1422 ops/sec single-GPU (2.2$\times$ improvement).
    \item \textbf{Efficiency trade-off}: Per-GPU efficiency decreases with more GPUs (56.8\% at 4 GPUs vs 75.1\% at 2 GPUs) due to thread synchronization and PCIe bandwidth sharing.
\end{itemize}

% ==========================================================
\section{Latency Bottleneck Analysis}
To understand where time is spent in HE operations, I implemented comprehensive performance monitoring using CUDA events for precise GPU timing. The benchmark breaks down each phase of the HE pipeline.

\subsection{End-to-End Latency Breakdown}
\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrrr}
\toprule
\textbf{Phase} & \textbf{Time (ms)} & \textbf{\% Total} & \textbf{Location} \\
\midrule
CPU Encryption & 36,018 & 91.63\% & CPU (OpenFHE) \\
Key Generation & 1,887 & 4.80\% & CPU (OpenFHE) \\
H2D Transfer & 398 & 1.01\% & PCIe Bus \\
Context Setup & 391 & 0.99\% & CPU \\
Key Loading & 317 & 0.81\% & GPU Memory \\
\textbf{GPU Compute} & \textbf{297} & \textbf{0.76\%} & \textbf{GPU} \\
\midrule
\textbf{Total} & 39,307 & 100\% & --- \\
\bottomrule
\end{tabular}
\caption{End-to-end latency breakdown for 64 HE multiplications (logN=14, L=12, 5 iterations). CPU encryption dominates at 91.6\%.}
\label{tab:latency}
\end{table}

\textbf{Key Finding:} CPU-side encryption (performed by OpenFHE) dominates the end-to-end pipeline at 91.6\% of total time. The actual GPU compute accounts for only 0.76\% of wall-clock time, demonstrating that FIDESlib's GPU kernels are highly efficient.

\subsection{Implications for Real Workloads}
\begin{itemize}[noitemsep]
    \item \textbf{Pre-encryption strategy}: For production systems, data should be pre-encrypted before GPU batch processing to amortize encryption cost.
    \item \textbf{GPU compute is not the bottleneck}: Multi-GPU scaling provides the most benefit when operating on already-encrypted data.
    \item \textbf{Memory transfer overhead}: H2D transfers (1.01\%) are minimal compared to CPU encryption, validating the GPU-accelerated approach.
\end{itemize}

% ==========================================================
\section{Pure GPU Compute Performance (Within-Node)}
To isolate the true multi-GPU scaling efficiency, I measured only the GPU compute phase, excluding CPU encryption, key generation, and memory transfers. This represents the performance users will observe when operating on pre-encrypted ciphertexts.

\subsection{GPU-Only Timing Results}
\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrrrrc}
\toprule
\textbf{Config (ops$\times$iters)} & \textbf{1 GPU} & \textbf{4 GPU} & \textbf{Speedup} & \textbf{Eff.} & \textbf{Imbalance} \\
\midrule
32$\times$5 (160 ops) & 140.1 ms & 64.1 ms & \textbf{2.19$\times$} & 54.7\% & 16.1\% \\
64$\times$5 (320 ops) & 296.8 ms & 121.3 ms & \textbf{2.45$\times$} & 61.2\% & 2.9\% \\
\bottomrule
\end{tabular}
\caption{Pure GPU compute scaling (logN=14, L=12). Larger workloads achieve better efficiency due to reduced synchronization overhead.}
\label{tab:gpu-only}
\end{table}

\subsection{Per-GPU Load Balance}
With 64 operations distributed across 4 GPUs (16 ops each):
\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{GPU} & \textbf{Time (ms)} & \textbf{Ops Assigned} \\
\midrule
GPU 0 & 117.9 & 16 \\
GPU 1 & 120.6 & 16 \\
GPU 2 & 120.8 & 16 \\
GPU 3 & 117.3 & 16 \\
\midrule
\textbf{Average} & 119.2 & --- \\
\textbf{Imbalance} & 2.93\% & --- \\
\bottomrule
\end{tabular}
\caption{Per-GPU execution times showing excellent load balance.}
\end{table}

\subsection{Throughput Comparison}
\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Configuration} & \textbf{1 GPU} & \textbf{4 GPU} \\
\midrule
HE-mults/sec (32 ops) & 1,142 & 2,498 \\
HE-mults/sec (64 ops) & 1,078 & 2,639 \\
\bottomrule
\end{tabular}
\caption{GPU compute throughput (excluding CPU overhead).}
\end{table}

\subsection{Scaling Efficiency Analysis}
The measured efficiency of 61.2\% at 4 GPUs (for larger workloads) is attributed to:
\begin{itemize}[noitemsep]
    \item \textbf{Thread synchronization}: Each GPU runs in a separate CPU thread; barrier synchronization adds overhead.
    \item \textbf{PCIe contention}: All 4 GPUs share the PCIe bus for memory operations.
    \item \textbf{Context switching}: CUDA runtime overhead when managing multiple devices.
\end{itemize}

\textbf{Workload scaling}: Larger workloads (64 ops vs 32 ops) improve efficiency from 54.7\% to 61.2\% because the fixed synchronization overhead is amortized over more compute operations.

% ==========================================================
\section{Multi-GPU Usage Pattern}
The implemented solution allows users to create independent GPU contexts:
\begin{lstlisting}
// Create separate contexts for each GPU
FIDESlib::CKKS::Context ctx0(params, {0}); // GPU 0
FIDESlib::CKKS::Context ctx1(params, {1}); // GPU 1

// Each context has independent keys
ctx0.AddEvalKey(std::move(eval_key_gpu0));
ctx1.AddEvalKey(std::move(eval_key_gpu1));

// Operations run independently on each GPU
std::thread t0([&]{ ct0->mult(*ct0, ctx0.GetEvalKey()); });
std::thread t1([&]{ ct1->mult(*ct1, ctx1.GetEvalKey()); });
\end{lstlisting}

% ==========================================================
\section{Files Modified}
The following source files were modified to enable multi-GPU support:
\begin{itemize}[noitemsep]
    \item \texttt{include/CKKS/Context.cuh}: Added per-context key storage members
    \item \texttt{src/CKKS/Context.cu}: Implemented instance-based key methods
    \item \texttt{src/CKKS/Limb.cu}: Device-aware graph cache (2 locations)
    \item \texttt{src/CKKS/LimbPartition.cu}: Device-aware graph cache (2 locations)
    \item \texttt{src/ConstantsGPU.cu}: GPU ID indexing fix, initialization tracking
\end{itemize}

% ==========================================================
\section{Computation Graph for HE MatMul}
The logical flow of the implemented parallel MatMul is as follows:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/ComputationGraphHEMatMul.png}
    \caption{Computation graph of HE MatMul operation showing data-parallel distribution across GPUs.}
    \label{fig:HEMatMul}
\end{figure}


% ==========================================================
\section{Conclusion}
This project successfully transformed FIDESlib into a multi-GPU capable library. The key contributions are:
\begin{enumerate}[noitemsep]
    \item \textbf{Architectural refactoring}: Converted static global state to per-context instance members, enabling independent GPU operation.
    \item \textbf{Device-aware caching}: Modified CUDA graph caches to include device ID, preventing cross-device corruption.
    \item \textbf{Robust initialization}: Implemented GPU tracking to ensure proper constant memory setup on all devices.
    \item \textbf{Comprehensive benchmarking}: Developed a benchmark suite demonstrating \textbf{2.45$\times$ speedup with 4 GPUs} on pure GPU compute.
    \item \textbf{Performance analysis}: Identified CPU encryption as the dominant bottleneck (91.6\% of end-to-end time), validating the importance of GPU acceleration for compute-bound operations.
\end{enumerate}

The implementation achieves 61.2\% parallel efficiency on 4 GPUs with excellent load balance (2.93\% imbalance). GPU compute throughput scales from 1,078 HE-mults/sec (1 GPU) to 2,639 HE-mults/sec (4 GPUs). Future work could explore NVLink interconnects for reduced communication overhead and GPU-side encryption to eliminate the CPU bottleneck.

\textbf{Repository:} \href{https://github.com/hkanpak21/FIDESlib}{github.com/hkanpak21/FIDESlib}

% ==========================================================
{\footnotesize
\begin{thebibliography}{9}\setlength{\itemsep}{1pt}
\bibitem{fides-paper}
C.~Agull{\'o}-Domingo et al., ``FIDESlib: A Fully-Fledged Open-Source FHE Library for Efficient CKKS on GPUs,'' arXiv:2507.04775 (2025).
\bibitem{fides-github}
\href{https://github.com/CAPS-UMU/FIDESlib}{https://github.com/CAPS-UMU/FIDESlib}
\end{thebibliography}
}

\end{document}


